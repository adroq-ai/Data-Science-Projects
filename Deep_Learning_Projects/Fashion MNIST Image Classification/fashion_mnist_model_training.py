# -*- coding: utf-8 -*-
"""Fashion_MNIST_Model_Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17FIJ-34DfyngCvUSILjC4CcW7vPJKuft

##**Fashion MNIST**
An MNIST-like dataset of 70,000 28x28 labeled fashion images

###**About Dataset**
####**Context**
Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of **60,000** examples and a test set of **10,000** examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.

The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. "If it doesn't work on MNIST, it won't work at all", they said. "Well, if it does work on MNIST, it may still fail on others."

Zalando seeks to replace the original MNIST dataset

###**Content**
Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.

* To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.
* For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.


###**Labels**

Each training and test example is assigned to one of the following labels:


* **0** T-shirt/top
* **1** Trouser
* **2** Pullover
* **3** Dress
* **4** Coat
* **5** Sandal
* **6** Shirt
* **7** Sneaker
* **8** Bag
* **9** Ankle boot


TL;DR

* Each row is a separate image
* Column 1 is the class label.
* Remaining columns are pixel numbers (784 total).
* Each value is the darkness of the pixel (1 to 255)

###**Acknowledgements**
* Original dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist

* Dataset was converted to CSV with this script: https://pjreddie.com/projects/mnist-in-csv/

###**License**
The MIT License (MIT) Copyright Â© [2017] Zalando SE, https://tech.zalando.com

##**Install necessary Libraries**
"""

# Install the Kaggle and Gradio Library
!pip install kaggle gradio

"""##**Import the Libraries**"""

import numpy as np
import pandas as pd
import random
from PIL import Image
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras, metrics
from tensorflow.keras import datasets, layers, models, optimizers
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, LeakyReLU
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.models import load_model, Sequential
from tensorflow.math import confusion_matrix
import cv2
from google.colab.patches import cv2_imshow
import gradio as gr

"""##**Seeding for reproducibility**"""

# Set seeds for reproducibility
random.seed(0)

np.random.seed(0)

tf.random.set_seed(0)

"""###**Data Preparation**"""

# Install dependencies as needed:
# pip install kagglehub[pandas-datasets]
import kagglehub
from kagglehub import KaggleDatasetAdapter

# Set the path to the file you'd like to load
file_path = "fashion-mnist_train.csv" # Changed to point to an actual file within the dataset

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "zalando-research/fashionmnist",
  file_path,
  # Provide any additional arguments like
  # sql_query or pandas_kwargs. See the
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

print("First 5 records:", df.head())

# Create a 4*4 grid of images
fig, axes = plt.subplots(4,4, figsize=(8,8))
fig.suptitle("First 16 Images", fontsize = 16)

# Plot the first 16 images from the dataset
for i, ax in enumerate(axes.flat):
    img = df.iloc[i, 1:].values.reshape(28,28)
    ax.imshow(img)   # Display in grayscale
    ax.axis('off')   # Remove axis for a clear look
    ax.set_title(f"Label: {df.iloc[i, 0]}")  # Show the label

plt.tight_layout(rect= [0,0,1,0.96])   # Adjust layout to fit the title
plt.show()

# Load and prepare the Fashion MNIST dataset
fashion_mnist = datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Create a 4x4 grid of images
fig, axes = plt.subplots(4, 5, figsize=(8, 8))
fig.suptitle("First 20 Images", fontsize=16)

# Plot the first 16 images from the dataset
for i, ax in enumerate(axes.flat):
    img = train_images[i]
    ax.imshow(img)   # Display in grayscale
    ax.axis('off')   # Remove axis for a clear look
    ax.set_title(f"Label: {train_labels[i]}")  # Show the label

plt.tight_layout(rect= [0, 0 , 1, 0.96])   # Adjust layout to fit the title
plt.show()

"""###**Data Processing**"""

type(train_images)

type(train_labels)

print(len(train_images))

print(len(train_labels))

print(len(test_images))

"""**class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']**"""

print(train_images[0])

# Display an image from the dataset
plt.imshow(train_images[0], cmap='gray')
plt.show()

print(train_labels[0])

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

print(train_images[0])

# Reshape images to specify that it's a single channel (grayscale)
train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))
test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))

# The training set contains 60,000 grayscale images of size 28x28 pixels with 1 color channel
# Shape format: (num_samples, height, width, channels)
print(train_images.shape)  # Output: (60000, 28, 28, 1)

# The test set contains 10,000 grayscale images of size 28x28 pixels with 1 color channel
print(test_images.shape)   # Output: (10000, 28, 28, 1)

"""###**Building Simple CNN model for classifying**"""

# Reshape input data to match CNN expected input shape
train_images = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0
test_images = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# Building the convolutional base model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.Dropout(0.3),  # NEW

    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.4),  # NEW

    layers.Dense(10, activation='softmax')
])

# âš™ï¸ Compile with Optimized Settings
model.compile(
    optimizer=  keras.optimizers.Adam(),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Add callbacks
callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1),
    ModelCheckpoint('best_classifier_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

print("train_images shape:",train_images.shape)
print("train_labels shape:", train_labels.shape)
print("Unique labels:", np.unique(train_labels))

# ðŸ‹ï¸â€â™‚ï¸ Train the CNN classifier
history_1 = model.fit(
    train_images, train_labels,
    epochs=20,
    batch_size=128,
    shuffle=True,
    validation_data=(test_images, test_labels),
    callbacks=callbacks
)

# Model summary
model.summary()

"""###**Model Evaluation**"""

# Evaluate the model
test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

# Plot training & validation accuracy values
plt.plot(history_1.history['accuracy'], label='Train Accuracy')
plt.plot(history_1.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.show()

# Plot training & validation loss values
plt.plot(history_1.history['loss'], label='Train Loss')
plt.plot(history_1.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Get predictions from the model
predictions = model.predict(test_images)

# Convert predictions to class labels
predicted_labels = np.argmax(predictions, axis=1)

# Generate the confusion matrix
conf_mat = confusion_matrix(test_labels, predicted_labels)
print(conf_mat)

# Create the heatmap with enhancements
plt.figure(figsize=(15, 10))
sns.heatmap(
    conf_mat,
    annot=True,
    fmt="d",  # integers
    cmap="coolwarm",
    cbar_kws={"shrink": 0.8},
    linewidths=0.5,
    square=True,
    annot_kws={"size": 8, "weight": "bold"},
)
plt.title("Enhanced Correlation Heatmap", fontsize=14, fontweight="bold", pad=15)
plt.xticks(fontsize=10, weight="bold")
plt.yticks(fontsize=10, weight="bold")
plt.tight_layout()
plt.ylabel("True Labels", fontsize=12, fontweight="bold")
plt.xlabel("Predicted Labels", fontsize=12, fontweight="bold")
plt.show()

"""###**Building Predictive System**"""

# Fashion MNIST class names
class_names = [
    "T-shirt/top ðŸ‘•", "Trouser ðŸ‘–", "Pullover ðŸ§¥", "Dress ðŸ‘—", "Coat ðŸ§¥",
    "Sandal ðŸ©´", "Shirt ðŸ‘”", "Sneaker ðŸ‘Ÿ", "Bag ðŸ‘œ", "Ankle boot ðŸ‘¢"
]

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Building predictive model
def predict_fashion_image(image):
    """ðŸ§µ Predict the top 3 Fashion MNIST categories with confidence scores."""

    # ðŸ–¤ Convert to grayscale (Fashion MNIST is grayscale)
    image_gray = image.convert('L')

    # ðŸ“ Resize to 28x28 pixels
    image_resized = image_gray.resize((28, 28))

    # ðŸ§® Convert to numpy array & Normalize pixel values to range [0, 1]
    img_array = np.array(image_resized) / 255.0

    # ðŸ”„ Reshape for the CNN model (batch_size, height, width, channels)
    img_input = img_array.reshape(1, 28, 28, 1)

    # ðŸ§  Make prediction
    prediction = model.predict(img_input)[0]

    # ðŸ“Š Get top 3 predictions with scores
    top_indices = prediction.argsort()[-3:][::-1]
    top_classes = [class_names[i] for i in top_indices]
    top_scores = [round(prediction[i] * 100, 2) for i in top_indices]

    # ðŸ“ Format top-3 as a dictionary (for bar plot display)
    result_dict = {top_classes[i]: top_scores[i] for i in range(3)}

    return result_dict

import gradio as gr

interface = gr.Interface(
    fn=predict_fashion_image,
    inputs=gr.Image(type="pil", label="ðŸ“· Upload Fashion Image"),
    outputs=gr.Label(num_top_classes=3, label="ðŸŽ¯ Top Predictions"),  # Show confidence scores as bar chart
    title="ðŸ‘— Fashion MNIST Predictor",
    description="Upload a grayscale image of a fashion item and get the top 3 predicted classes with confidence scores."
)

interface.launch(debug=True)

# Visualize Wrong Predictions
# Import the matplotlib library for plotting images
import matplotlib.pyplot as plt

# Define the label names for each class in the Fashion MNIST dataset
class_names = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

# Find the indices where the predicted labels do not match the true labels
# This gives us the indices of incorrect predictions
wrong = np.where(predicted_labels != test_labels)[0]

# Set the figure size for displaying the images (12x12 inches)
plt.figure(figsize=(12, 12))

# Loop through the first 9 wrong predictions and plot them
for i, index in enumerate(wrong[:9]):
    plt.subplot(3, 3, i + 1)  # Create a 3x3 grid of subplots and access the (i+1)th plot
    plt.imshow(test_images[index].reshape(28, 28), cmap='gray')  # Show the image in grayscale
    # Add a title showing the true and predicted class names
    plt.title(f"True: {class_names[test_labels[index]]}\nPredicted: {class_names[predicted_labels[index]]}")
    plt.axis('off')  # Turn off axis ticks for a cleaner look

# Adjust subplot layout to avoid overlapping elements
plt.tight_layout()

# Display the final plot
plt.show()

"""#####**Save the model to Google drive or local**"""

model.save('trained_fashion_mnist_model.h5')

model.save("drive/MyDrive/Saved Models/plant_disease_prediction_model.h5")

"""####**Building an Autoencoder to train reconstruct input images**"""

def build_autoencoder(activation_name='relu'):
    # Select activation
    if activation_name == 'leaky_relu':
        activation = LeakyReLU(alpha=0.3)
    elif activation_name == 'relu':
        activation = 'relu'
    else:
        raise ValueError("Unsupported activation. Use 'relu' or 'leaky_relu'.")

    model = Sequential()

    # ðŸ§  Encoder
    model.add(Conv2D(32, (3, 3), activation=activation, padding='same', input_shape=(28, 28, 1)))
    model.add(MaxPooling2D((2, 2), padding='same'))

    model.add(Conv2D(64, (3, 3), activation=activation, padding='same'))
    model.add(MaxPooling2D((2, 2), padding='same'))

    model.add(Conv2D(128, (3, 3), activation=activation, padding='same'))

    # ðŸ§± Decoder
    model.add(UpSampling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation=activation, padding='same'))

    model.add(UpSampling2D((2, 2)))
    model.add(Conv2D(32, (3, 3), activation=activation, padding='same'))

    # ðŸŽ¯ Output Layer
    model.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))

    return model  # âœ… Return the model

# âœ… Build the model
autoencoder = build_autoencoder('leaky_relu')

# Autoencoder summary
autoencoder.summary()

# âš™ï¸ Compile with Optimized Settings
autoencoder.compile(
    optimizer=  keras.optimizers.Adam(),
    loss='binary_crossentropy',
    metrics=['accuracy',
             keras.metrics.Precision(name='precision'),
             keras.metrics.Recall(name='recall')]
)

# Add callbacks
callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1),
    ModelCheckpoint('best_autoencoder.h5', monitor='val_loss', save_best_only=True, verbose=1)
]

# ðŸ‹ï¸â€â™‚ï¸ Train the autoencoder
history = autoencoder.fit(
    train_images, train_images,  # Note: input == output for autoencoders
    epochs=10,
    batch_size=128,
    shuffle=True,
    validation_data=(test_images, test_images),
    callbacks=callbacks
)

print(autoencoder.history.history.keys())

print(history.history['accuracy'])  # âœ… Training accuracy for each epoch
print(history.history['val_loss'])  # âœ… Validation loss for each epoch

"""####**Model Evaluation**"""

# Evaluate the model
decoded_imgs = autoencoder.predict(test_images)
test_loss = tf.keras.losses.binary_crossentropy(test_images.reshape(-1, 28*28), decoded_imgs.reshape(-1, 28*28)).numpy().mean()
print('\nTest loss:', test_loss)  # Print the calculated test loss

# Evaluate the autoencoder by comparing inputs to reconstructed outputs
test_loss = autoencoder.evaluate(test_images, test_images, verbose=2)

print('\nReconstruction Loss (Test):', test_loss)

# âœ… Define a threshold value for binarization (pixel values > 0.5 become 1, else 0)
threshold = 0.5

# âœ… Flatten both the original test images and reconstructed images
# - Reshape from (num_samples, 28, 28) to (num_samples, 784) for easier comparison
original = test_images.reshape(-1, 28*28)
reconstructed = decoded_imgs.reshape(-1, 28*28)

# âœ… Binarize the original images (convert to 0 or 1)
# - Any pixel > 0.5 becomes 1, else becomes 0
original_bin = (original > threshold).astype(int)

# âœ… Binarize the reconstructed images using the same threshold
reconstructed_bin = (reconstructed > threshold).astype(int)

# âœ… Calculate pixel-wise accuracy
# - Compares each pixel value (0 or 1) between original and reconstructed images
# - Computes mean to get the percentage of correctly reconstructed pixels
reconstruction_accuracy = np.mean(original_bin == reconstructed_bin)

# âœ… Print the reconstruction accuracy
print("Reconstruction Accuracy:", reconstruction_accuracy)

import matplotlib.pyplot as plt

# Get predictions from the trained autoencoder
decoded_imgs = autoencoder.predict(test_images)

# Number of images to display
n = 10

plt.figure(figsize=(20, 4))

for i in range(n):
    # ðŸ”µ Original Image
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(test_images[i].reshape(28, 28), cmap="gray")
    plt.title("Original")
    plt.axis("off")

    # ðŸŸ£ Reconstructed Image
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap="gray")
    plt.title("Reconstructed")
    plt.axis("off")

plt.tight_layout()
plt.show()

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='center right')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Get predictions from the model
predictions = autoencoder.predict(test_images)

# Predict a single image for anomaly detection
def detect_anomaly(image):
    # âœ… Step 1: Preprocess the image
    img = image.convert('L').resize((28, 28))  # Convert image to grayscale (L mode) and resize to 28x28 pixels
    img_array = np.array(img).astype('float32') / 255.0  # Convert image to NumPy array and normalize pixel values to [0, 1]
    img_input = img_array.reshape(1, 28, 28, 1)  # Reshape to match model input shape (batch_size, height, width, channels)

    # âœ… Step 2: Use the autoencoder to reconstruct the image
    reconstructed = autoencoder.predict(img_input)  # Output will have same shape as input

    # âœ… Step 3: Compute Mean Squared Error (MSE) between input and reconstruction
    mse = np.mean((img_input - reconstructed) ** 2)  # High MSE means reconstruction failed = anomaly

    # âœ… Step 4: Compare MSE to a manually defined threshold
    threshold = 0.01  # Threshold for what is considered a "normal" image; tweak this based on testing

    # âœ… Step 5: Return the result based on MSE comparison
    status = "âœ… Normal" if mse < threshold else "ðŸš¨ Anomaly"  # Classify the input
    return f"{status} | Reconstruction Error: {mse:.5f}"  # Return result with formatted MSE

import gradio as gr

interface = gr.Interface(
    fn=detect_anomaly,
    inputs=gr.Image(type="pil", label="ðŸ–¼ï¸ Upload Grayscale Image"),
    outputs=gr.Textbox(label="Anomaly Status"),
    title="ðŸ” Autoencoder Anomaly Detector",
    description="Detect whether an image is normal or anomalous based on reconstruction error using an autoencoder."
)

interface.launch(debug=True)

"""####**Building another CNN Model Architechecture**"""

# Build the convolutional base
model_2 = models.Sequential()
model_2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model_2.add(layers.MaxPooling2D((2, 2)))
model_2.add(layers.Conv2D(64, (3, 3), activation='relu'))
model_2.add(layers.MaxPooling2D((2, 2)))
model_2.add(layers.Conv2D(64, (3, 3), activation='relu'))

# Add Dense layers on top
model_2.add(layers.Flatten())
model_2.add(layers.Dense(64, activation='relu'))
model_2.add(layers.Dense(10))


# Compile and train the model
model_2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


history_2 = model_2.fit(train_images, train_labels, epochs=5,
                    validation_data=(test_images, test_labels))


# Evaluate the model
test_loss, test_acc = model_2.evaluate(test_images,  test_labels, verbose=2)
print('\nTest accuracy:', test_acc)

# Plot training & validation accuracy values
plt.plot(history_2.history['accuracy'], label='Train Accuracy')
plt.plot(history_2.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.show()

# Plot training & validation loss values
plt.plot(history_2.history['loss'], label='Train Loss')
plt.plot(history_2.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

# Get predictions from the model
predictions_2 = model_2.predict(test_images)

# Convert predictions to class labels
predicted_labels_2 = np.argmax(predictions_2, axis=1)

# Generate the confusion matrix
conf_mat_2 = confusion_matrix(test_labels, predicted_labels_2)
print(conf_mat)

# Create the heatmap with enhancements
plt.figure(figsize=(15, 10))
sns.heatmap(
    conf_mat_2,
    annot=True,
    fmt="d",  # integers
    cmap="coolwarm",
    cbar_kws={"shrink": 0.8},
    linewidths=0.5,
    square=True,
    annot_kws={"size": 8, "weight": "bold"},
)
plt.title("Enhanced Correlation Heatmap", fontsize=14, fontweight="bold", pad=15)
plt.xticks(fontsize=10, weight="bold")
plt.yticks(fontsize=10, weight="bold")
plt.tight_layout()
plt.ylabel("True Labels", fontsize=12, fontweight="bold")
plt.xlabel("Predicted Labels", fontsize=12, fontweight="bold")
plt.show()

def predict_fashion_image(image):
    """ðŸ§µ Predict the Fashion MNIST category from an uploaded image."""

    # ðŸ–¤ Convert to grayscale (Fashion MNIST is grayscale)
    image_gray = image.convert('L')

    # ðŸ“ Resize to 28x28 pixels
    image_resized = image_gray.resize((28, 28))

    # ðŸ§® Convert to numpy array
    image_array = np.array(image_resized)

    # ðŸ§¼ Normalize pixel values to range [0, 1]
    img_normalized = image_array / 255.0

    # ðŸ”„ Reshape for the CNN model (batch_size, height, width, channels)
    img_reshaped = img_normalized.reshape(1, 28, 28, 1)

    # ðŸ§  Make prediction
    prediction = model_2.predict(img_reshaped)

    # ðŸŽ¯ Get the predicted class index
    predicted_label = np.argmax(prediction)

    # ðŸ·ï¸ Map label to class name (if available)
    class_names = [
        "T-shirt/top ðŸ‘•", "Trouser ðŸ‘–", "Pullover ðŸ§¥", "Dress ðŸ‘—", "Coat ðŸ§¥",
        "Sandal ðŸ©´", "Shirt ðŸ‘”", "Sneaker ðŸ‘Ÿ", "Bag ðŸ‘œ", "Ankle boot ðŸ‘¢"
    ]
    predicted_class = class_names[predicted_label]

    return f"ðŸ‘š Predicted Fashion Item: {predicted_class}"

import gradio as gr

interface = gr.Interface(
    fn=predict_fashion_image,
    inputs=gr.Image(type="pil", label="ðŸ“¤ Upload a Fashion Image"),
    outputs=gr.Textbox(label="ðŸŽ¯ Predicted Category"),
    title="ðŸ§µ Fashion MNIST Classifier",
    description="Upload a grayscale fashion image (28x28 or larger) and the model will predict its category (e.g., ðŸ‘—, ðŸ‘•, ðŸ‘¢)."
)

# ðŸš€ Launch the App
interface.launch(debug=True)