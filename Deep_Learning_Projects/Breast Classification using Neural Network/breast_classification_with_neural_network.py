# -*- coding: utf-8 -*-
"""Breast Classification with Neural Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mRG1fBboKAKIYV4IZxAqc3K7vHMgA2G8

#**About Dataset**

Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.

**n** the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: "Robust Linear Programming Discrimination of Two Linearly Inseparable Sets", Optimization Methods and Software 1, 1992, 23-34].

This database is also available through the **UW CS ftp server:**

**ftp ftp.cs.wisc.edu**

**cd math-prog/cpo-dataset/machine-learn/WDBC/**

Also can be found on UCI Machine Learning Repository:

 https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29

**Attribute Information:**

1) ID number

2) Diagnosis (M = malignant, B = benign)

3-32)

Ten real-valued features are computed for each cell nucleus:

a) radius (mean of distances from center to points on the perimeter)

b) texture (standard deviation of gray-scale values)

c) perimeter

d) area

e) smoothness (local variation in radius lengths)

f) compactness (perimeter^2 / area - 1.0)

g) concavity (severity of concave portions of the contour)

h) concave points (number of concave portions of the contour)

i) symmetry

j) fractal dimension ("coastline approximation" - 1)

The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance,

**field 3 is Mean Radius,**

**field 13 is Radius SE,**

**field 23 is Worst Radius.**

All feature values are recoded with four significant digits.

**Missing attribute values:**  **none**


**Class distribution:**

**357 benign,**

**212 malignant**

#**Import the Libraries**
"""

## import some basic libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""#**Data Collection and Preprocessing**"""

# Loading the data from sklearn
breast_cancer_data = load_breast_cancer()
print(breast_cancer_data)

# Loading the dataset to a DataFrame
df = pd.DataFrame(breast_cancer_data.data, columns = breast_cancer_data.feature_names)

# Display the first few rows
print("First 5 rows of the dataset:")
df.head()

# Adding the 'target' column to the data frame
df['label'] = breast_cancer_data.target

# Display the last few rows
print("last 5 rows of the dataset:")
df.tail()

# Numbers of rows and columns in the dataset
df.shape   # Outputs the number of rows and columns in the dataset.

# Getting some informations about the datasets
df.info()

# Check for missing values
missing_values = df.isnull().sum()
print("\nMissing values in each column:")
print(missing_values)

# Statistical description of the dataset
print("\nStatistical Description of the Dataset:")
df.describe()

# Checking the distribution of Target Variable
print("\nStatistical Distribution of the Dataset:")
df['label'].value_counts()

"""**1 ---> Benign**

**0 ---> Malignant**
"""

# Checking the Mean distribution of Target Variable
print("\nMean Distribution of the Dataset:")
df.groupby('label').mean()

"""**Splitting the dataset into Features & Target**"""

# Splitting the dataset into Features & Target
X = df.drop(columns="label", axis=1)
y = df["label"]

print("Features (X):")
print(X.head())
print("\nTarget (y):")
print(y.head())

"""#**Splitting the dataset into Training and Test sets**"""

# Splitting the dataset into Training and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)
print("\nDataset split completed:")
print(f"Total samples: {X.shape[0]}, Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

# checking the number of Test and Train dataset
print(X.shape, X_train.shape, X_test.shape)

"""**Standardize the Data**

To have a better accuracy
"""

# Standardize the Data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)

"""#**Building Neural Network**"""

# Importing tensorflow and Keras
import tensorflow as tf  #  Imports TensorFlow for deep learning tasks.
tf.random.set_seed(2)    #  Ensures reproducibility by setting a fixed random seed.
from tensorflow import keras # Imports Keras, a high-level API for building models.

"""**Defining the Neural Network**"""

# Setting up the layers of Neural Network i.e layers are stacked in order.
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(30,)),     # Input layer
    keras.layers.Dense(15, activation='relu'),   # Hidden layer
    keras.layers.Dense(2, activation='sigmoid')  # Output layer  (for binary classification)
])

"""**Compiling the Model**"""

# Compiling the Neural Network
model.compile(optimizer='adam',    # Adapts the learning rate dynamically for efficient training
              loss='sparse_categorical_crossentropy',   # Suitable for multi-class classification with integer labels
              metrics=['accuracy'])   # Tracks accuracy as a performance measure.

"""**Training the Neural Network**"""

# Training the Neural Network
history = model.fit(X_train_std, y_train, validation_split=0.2, epochs=30)

"""#**Visualizing the Accuracy and Loss**"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training Data', 'Validation Data'], loc='lower right')
plt.show

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training Data', 'Validation Data'], loc='upper right')
plt.show

"""#**Accuracy of the model on the test data**"""

loss, accuracy = model.evaluate(X_test_std, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

print(X_test_std.shape)
print(X_test_std[0])

print(X_test_std)

y_pred = model.predict(X_test_std)
print(y_pred.shape)

print(y_pred)

print(y_pred[0])

"""**model.predict() gives the prediction probability of each class for that data point**"""

# Example on argmax function
my_list = [0.8, 0.2]
max_index = np.argmax(my_list)
print(max_index)

# Converting the prediction probability to class labels
y_pred_labels = np.argmax(y_pred, axis=1)
print(y_pred_labels)

"""**OR**"""

# Converting the prediction probability to class labels
y_pred_labels = [np.argmax(pred) for pred in y_pred]
print(y_pred_labels)

"""#**Building the Predictive System**"""

input_data = [13,21.82,87.5,519.8,0.1273,0.1932,0.1859,0.09353,0.235,0.07389,0.3063,1.002,2.406,24.32,0.005731,0.03502,0.03553,0.01226,0.02143,0.003749,15.49,30.73,106.2,739.3,0.1703,0.5401,0.539,0.206,0.4378,0.1072]

# Converting input data to a NumPy array
input_data_as_numpy_array = np.asarray(input_data)

# Reshaping the numpy array as we are predicting for one data point
input_data_reshaped = input_data_as_numpy_array.reshape(1, -1)

# Standardize the input data
std_data = scaler.transform(input_data_reshaped)

# Making a prediction
prediction = model.predict(std_data)
print(prediction)

# Converting the prediction probability to class label
predicted_class = np.argmax(prediction)
print(predicted_class)

if predicted_class == 0:
    print("The Breast Cancer(Tumor) is Malignant")
else:
    print("The Breast Cancer(Tumor) is Benign")

input_data = [8.196,16.84,51.71,201.9,0.086,0.05943,0.01588,0.005917,0.1769,0.06503,0.1563,0.9567,1.094,8.205,0.008968,0.01646,0.01588,0.005917,0.02574,0.002582,8.964,21.96,57.26,242.2,0.1297,0.1357,0.0688,0.02564,0.3105,0.07409]

# Converting input data to a NumPy array
input_data_as_numpy_array = np.asarray(input_data)

# Reshaping the numpy array as we are predicting for one data point
input_data_reshaped = input_data_as_numpy_array.reshape(1, -1)

# Standardize the input data
std_data = scaler.transform(input_data_reshaped)

# Making a prediction
prediction = model.predict(std_data)
print(prediction)

# Converting the prediction probability to class label
predicted_class = np.argmax(prediction)
print(predicted_class)

if predicted_class == 0:
    print("The Breast Cancer(Tumor) is Malignant")
else:
    print("The Breast Cancer(Tumor) is Benign")