# -*- coding: utf-8 -*-
"""Calories Burnt Prediction using XGBRegressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZAl2UfQF5BNyDwyHWL47S945_IlQcMWO

#**Downgrade scikit-learn**
"""

!pip install scikit-learn==1.5.2

"""#**Import the Libraries**"""

## import some basic libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn import metrics

"""#**Data Collection and Preprocessing**"""

# Loading the dataset into a pandas DataFrame
calories_data = pd.read_csv('calories.csv')  # Use this to identify issues

# Loading the dataset into a pandas DataFrame
exercise_data = pd.read_csv('exercise.csv')  # Use this to identify issues

# Display the first few rows
print("First 5 rows of the dataset:")
calories_data.head()

# Display the first few rows
print("First 5 rows of the dataset:")
exercise_data.head()

# Display the last few rows
print("Last 5 rows of the dataset:")
calories_data.tail()

# Display the last few rows
print("Last 5 rows of the dataset:")
exercise_data.tail()

"""#**Concatenating the two Dataframes**"""

calories_burnt_data = pd.concat([exercise_data, calories_data['Calories']], axis=1)

# Display the first few rows
print("First 5 rows of the dataset:")
calories_burnt_data.head()

# Checking the shape of the dataset
calories_burnt_data.shape    # Outputs the number of rows and columns in the dataset.

# Statistical description of the dataset
print("\nStatistical Description of the Dataset:")
calories_burnt_data.describe()

# Check for missing values
missing_values = calories_burnt_data.isnull().sum()
print("\nMissing values in each column:")
print(missing_values)

# Geerating some Information about the dataset
calories_burnt_data.info()

"""#**Data Analysis**"""

# Statistical description of the dataset
print("\nStatistical Description of the Dataset:")
calories_burnt_data.describe()

"""#**Data Visualization**"""

sns.set()

# Distribution Vertical Count Plot graph of Gender Column
plt.figure(figsize=(6,6))
sns.countplot(x='Gender', data=calories_burnt_data)
plt.title("Gender Distribution")
plt.show()

# Distribution graph of age value
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(calories_burnt_data["Age"])
plt.title("Age Distribution")
plt.show()

# Distribution graph of Height value
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(calories_burnt_data["Height"])
plt.title("Height Distribution")
plt.show()

# Distribution graph of Weight value
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(calories_burnt_data["Weight"])
plt.title("Weight Distribution")
plt.show()

# Distribution graph of Duration value
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(calories_burnt_data["Duration"])
plt.title("Duration Distribution")
plt.show()

"""#**Correlation of the datasets**"""

# Convert 'Gender' column to numerical representation using one-hot encoding
calories_burnt_dummies_data = pd.get_dummies(calories_burnt_data, columns=['Gender'], drop_first=True)

# Calculate the correlation matrix
corr = calories_burnt_dummies_data.corr()

# Print the correlation matrix
print(corr)

# Create the heatmap with enhancements
plt.figure(figsize=(10, 10))
sns.heatmap(
    corr,
    annot=True,
    fmt=".2f",
    cmap="coolwarm",
    cbar_kws={"shrink": 0.8},
    linewidths=0.5,
    square=True,
    annot_kws={"size": 8, "weight": "bold"},
)
plt.title("Enhanced Correlation Heatmap of Calories Burnt", fontsize=14, fontweight="bold", pad=15)
plt.xticks(fontsize=10, rotation=45, ha="right", weight="bold")
plt.yticks(fontsize=10, weight="bold")
plt.tight_layout()

"""#**Data Preprocessing**


**Encoding the Categorical column**
"""

# Converting the text data into numerical values
calories_burnt_data.replace({"Gender":{'male':0,'female':1}}, inplace=True)

# Display the first few rows
print("First 5 rows of the dataset:")
calories_burnt_data.head()

"""**Splitting the dataset into Features & Target**"""

# Splitting the dataset into Dependents & Independent
X = calories_burnt_data.drop(columns=['User_ID','Calories'], axis=1)
y = calories_burnt_data['Calories']

print("Features (X):")
print(X.head())
print("\nTarget (y):")
print(y.head())

"""# **Splitting the dataset into Training and Test sets**"""

# Splitting the dataset into Training and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=3)
print("\nDataset split completed:")
print(f"Total samples: {X.shape[0]}, Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

# checking the number of Test and Train dataset
print(X.shape, X_train.shape, X_test.shape)

"""# **Model Training**"""

# Initialize the model
model = XGBRegressor()

# Training the XGBRegressor model with train data
model.fit(X_train, y_train)

"""#**Model Evaluation**"""

# Evaluate on Training Data
training_data_prediction = model.predict(X_train)
r2_train = metrics.r2_score(y_train, training_data_prediction)
mae_train = metrics.mean_absolute_error(y_train, training_data_prediction)
print(f"\nTraining Data Evaluation:\nR-squared Error: {r2_train:.4f}\nMean Absolute Error: {mae_train:.4f}")

# Evaluate on Test Data
test_data_prediction = model.predict(X_test)
r2_test = metrics.r2_score(y_test, test_data_prediction)
mae_test = metrics.mean_absolute_error(y_test, test_data_prediction)
print(f"\nTest Data Evaluation:\nR-squared Error: {r2_test:.4f}\nMean Absolute Error: {mae_test:.4f}")