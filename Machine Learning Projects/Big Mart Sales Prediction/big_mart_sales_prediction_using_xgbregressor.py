# -*- coding: utf-8 -*-
"""Big Mart Sales Prediction using XGBRegressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DE0ai3Hc9AM_EBO9qR10JZZ53z38TM-0

#**Install necessary Libraries**
"""

# Use an Older Version of Scikit-learn
pip install scikit-learn==1.2.2

"""#**Import the Libraries**"""

## import some basic libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
from sklearn import metrics

"""#**Data Collection and Analysis**"""

# Loading the dataset into a pandas DataFrame
big_mart_sales_data = pd.read_csv('Train.csv')  # Use this to identify issues

# Display the first few rows
print("First 5 rows of the dataset:")
big_mart_sales_data.head()

# Display the last few rows
print("Last 5 rows of the dataset:")
big_mart_sales_data.tail()

# Checking the shape of the dataset
big_mart_sales_data.shape    # Outputs the number of rows and columns in the dataset.

# Statistical description of the dataset
print("\nStatistical Description of the Dataset:")
big_mart_sales_data.describe()

# Check for missing values
missing_values = big_mart_sales_data.isnull().sum()
print("\nMissing values in each column:")
print(missing_values)

# Geerating some Information about the dataset
big_mart_sales_data.info()

# Checking the distribution of data
print("\nDistribution of Data:")
print(big_mart_sales_data.value_counts())

"""**Categorical Features**



*   Item_Identifier
*   Item_Fat_Content
*   Item_Type
*   Outlet_Identifier
*   Outlet_Size
*   Outlet_Location_Type
*   Outlet_Type

## **Handling Missing Value**

Mean --> Average value

Mode --> Most occuring value

Replacing the missing values in "Item_weight" column with mean
"""

# Mean value of "Item_weight" column
big_mart_sales_data_mean = big_mart_sales_data['Item_Weight'].mean()

# Filling the mising values in "Item_weight" column with "mean value"
big_mart_sales_data['Item_Weight'].fillna(big_mart_sales_data_mean, inplace=True)

# Check for missing values
missing_values = big_mart_sales_data.isnull().sum()
print("\nMissing values in each column:")
print(missing_values)

"""Replacing the missing values in "Outlet_Size" column with mode"""

# Mode value of "Outlet_Size" column
mode_of_outlet_size = big_mart_sales_data.pivot_table(values='Outlet_Size',
                                                      columns=['Outlet_Type'],
                                                      aggfunc=(lambda x: x.mode()[0]))
print("\nMode value of 'Outlet_Size' column:")
print(mode_of_outlet_size)

missing_values_in_outlet_size = big_mart_sales_data['Outlet_Size'].isnull()
print("\nMissing values in 'Outlet_Size' column:")
print(missing_values_in_outlet_size)

big_mart_sales_data.loc[missing_values_in_outlet_size,
                        'Outlet_Size'] = big_mart_sales_data.loc[missing_values_in_outlet_size,
                                                                'Outlet_Type'].apply(lambda x: mode_of_outlet_size[x])
#

# Check for missing values
missing_values = big_mart_sales_data.isnull().sum()
print("\nMissing values in each column:")
print(missing_values)

"""# **Data Analysis**"""

# Statistical description of the dataset
print("\nStatistical Description of the Dataset:")
big_mart_sales_data.describe()

"""**Numerical Features**"""

sns.set()

# Distribution graph of Item_Weight column
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(big_mart_sales_data["Item_Weight"])
plt.title("Item Weight Distribution")
plt.show()

# Distribution graph of Item_Visibility column
sns.set()
plt.figure(figsize=(6,6))
sns.distplot(big_mart_sales_data["Item_Visibility"])
plt.title("Item Visibility Distribution")
plt.show()

# Distribution graph of Item_MRP column
plt.figure(figsize=(6,6))
sns.distplot(big_mart_sales_data["Item_MRP"])
plt.title("Item MRP Distribution")
plt.show()

# Distribution graph of Item_Outlet_Sales column
plt.figure(figsize=(6,6))
sns.distplot(big_mart_sales_data["Item_Outlet_Sales"])
plt.title("Item Outlet Sales Distribution")
plt.show()

# Distribution graph of Outlet_Establishment_Year Column
plt.figure(figsize=(6,6))
sns.countplot(x='Outlet_Establishment_Year', data=big_mart_sales_data)
plt.title("Outlet Establishment Year Distribution")
plt.show()

"""**Correlation**"""

numerical_data = big_mart_sales_data.select_dtypes(include=['number'])

corr = numerical_data.corr()


corr

# Create the heatmap with enhancements
plt.figure(figsize=(6, 6))
sns.heatmap(
    corr,
    annot=True,
    fmt=".2f",
    cmap="coolwarm",
    cbar_kws={"shrink": 0.8},
    linewidths=0.5,
    square=True,
    annot_kws={"size": 8, "weight": "bold"},
)
plt.title("Enhanced Correlation Heatmap", fontsize=10, fontweight="bold", pad=15)
plt.xticks(fontsize=10, rotation=45, ha="right", weight="bold")
plt.yticks(fontsize=10, weight="bold")
plt.tight_layout()

"""**Categorical Features**"""

# Distribution graph of Item_Fat_Content Column
plt.figure(figsize=(6,6))
sns.countplot(x='Item_Fat_Content', data=big_mart_sales_data)
plt.title("Item Fat Content Distribution")
plt.show()

# Distribution graph of Item_Type Column
plt.figure(figsize=(28,10))
sns.countplot(x='Item_Type', data=big_mart_sales_data)
plt.title("Item Type Distribution")
plt.show()

# Distribution graph of Outlet_Size Column
plt.figure(figsize=(1,6))
sns.countplot(x='Outlet_Size', data=big_mart_sales_data)
plt.title("Outlet Size Distribution")
plt.show()

# Distribution graph of Outlet_Location_Type Column
plt.figure(figsize=(6,6))
sns.countplot(x='Outlet_Location_Type', data=big_mart_sales_data)
plt.title("Outlet Location Type Distribution")
plt.show()

# Distribution graph of Outlet_Type  Column
plt.figure(figsize=(10,6))
sns.countplot(x='Outlet_Type', data=big_mart_sales_data)
plt.title("Outlet Type  Distribution")
plt.show()

"""# **Data Preprocessing**"""

# Display the first few rows
print("First 5 rows of the dataset:")
big_mart_sales_data.head()

# Checking the distribution of Item_Fat_Content data
print("\nDistribution of Data:")
big_mart_sales_data['Item_Fat_Content'].value_counts()

# Standardizing the values in the 'Item_Fat_Content' column to ensure consistency
# by replacing variations like 'low fat' and 'LF' with 'Low Fat', and 'reg' with 'Regular'.
big_mart_sales_data.replace({'Item_Fat_Content': {'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'}}, inplace=True)

# Checking the distribution of Item_Fat_Content data
print("\nDistribution of Data:")
big_mart_sales_data['Item_Fat_Content'].value_counts()

"""**Label Encoding**"""

# Initializing LabelEncoder()
encoder = LabelEncoder()

big_mart_sales_data['Item_Identifier'] = encoder.fit_transform(big_mart_sales_data['Item_Identifier'])

big_mart_sales_data['Item_Fat_Content'] = encoder.fit_transform(big_mart_sales_data['Item_Fat_Content'])

big_mart_sales_data['Item_Type'] = encoder.fit_transform(big_mart_sales_data['Item_Type'])

big_mart_sales_data['Outlet_Identifier'] = encoder.fit_transform(big_mart_sales_data['Outlet_Identifier'])

big_mart_sales_data['Outlet_Size'] = encoder.fit_transform(big_mart_sales_data['Outlet_Size'])

big_mart_sales_data['Outlet_Location_Type'] = encoder.fit_transform(big_mart_sales_data['Outlet_Location_Type'])

big_mart_sales_data['Outlet_Type'] = encoder.fit_transform(big_mart_sales_data['Outlet_Type'])

# Checking the distribution of Item_Fat_Content data
print("\nDistribution of Data:")
big_mart_sales_data['Item_Fat_Content'].value_counts()

# Display the first few rows
print("First 5 rows of the dataset:")
big_mart_sales_data.head()

"""**Splitting features and target**"""

# Splitting features and target
X = big_mart_sales_data.drop(columns='Item_Outlet_Sales', axis=1)
y = big_mart_sales_data['Item_Outlet_Sales']

print("Features (X):")
print(X.head())
print("\nTarget (y):")
print(y.head())

"""# **Splitting the dataset into Training and Test sets**"""

# Splitting the dataset into Training and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=5)
print("\nDataset split completed:")
print(f"Total samples: {X.shape[0]}, Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

# checking the number of Test and Train dataset
print(X.shape, X_train.shape, X_test.shape)

"""
# **Model Training**"""

# Initialize the model
xgb_model = XGBRegressor(random_state=1)

# Training the XGBRegressor model with train data
xgb_model.fit(X_train, y_train)

"""#**Model Evaluation**"""

# Evaluate on Training Data
training_data_prediction = xgb_model.predict(X_train)
r2_train = metrics.r2_score(y_train, training_data_prediction)
mae_train = metrics.mean_absolute_error(y_train, training_data_prediction)
print(f"\nTraining Data Evaluation:\nR-squared Error: {r2_train:.4f}\nMean Absolute Error: {mae_train:.4f}")

# Evaluate on Test Data
test_data_prediction = xgb_model.predict(X_test)
r2_test = metrics.r2_score(y_test, test_data_prediction)
mae_test = metrics.mean_absolute_error(y_test, test_data_prediction)
print(f"\nTest Data Evaluation:\nR-squared Error: {r2_test:.4f}\nMean Absolute Error: {mae_test:.4f}")